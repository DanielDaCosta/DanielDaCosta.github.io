<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://danieldacosta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danieldacosta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-29T06:07:11+00:00</updated><id>https://danieldacosta.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://danieldacosta.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://danieldacosta.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Set Up MLflow on AWS EC2 Using Docker, S3, and RDS</title><link href="https://danieldacosta.github.io/blog/2021/mlflow/" rel="alternate" type="text/html" title="Set Up MLflow on AWS EC2 Using Docker, S3, and RDS"/><published>2021-10-18T17:39:00+00:00</published><updated>2021-10-18T17:39:00+00:00</updated><id>https://danieldacosta.github.io/blog/2021/mlflow</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2021/mlflow/"><![CDATA[<p>If you are a data scientist, you definitely have felt the need to keep track of your machine learning models. If you are not careful, you can quickly lose track of the best model you have been tuning for hours.</p> <p>Numerous platforms can help you deal with this issue. In this post, we will focus on the open-source project called MLflow and how we implemented it to make our model training efforts more efficient.</p> <p>According to <a href="https://www.mlflow.org/docs/latest/index.html">MLflow’s documentation</a>:</p> <blockquote> <p>MLflow is an open source platform for managing the end-to-end machine learning lifecycle.</p> </blockquote> <p>The platform facilitates workflow management for training, tracking, and producing machine learning models.</p> <p>This post will not focus on what is MLflow and why you should use it because countless websites already explain that. Here, we will implement it on AWS using technologies, such as Docker, ECR, and EC2. We assume that you are already familiar with them.</p> <p>In the end, you will access and use the MLflow UI hosted on a remote server.</p> <hr/> <p>You can check the full blog post on my Medium: <a href="https://aws.plainenglish.io/set-up-mlflow-on-aws-ec2-using-docker-s3-and-rds-90d96798e555">Set Up MLflow on AWS EC2 Using Docker, S3, and RDS</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Implementation of MLflow to make model training efforts more efficient.]]></summary></entry><entry><title type="html">Building an ETL pipeline with Airflow and ECS</title><link href="https://danieldacosta.github.io/blog/2020/etl-airflow/" rel="alternate" type="text/html" title="Building an ETL pipeline with Airflow and ECS"/><published>2020-12-20T21:01:00+00:00</published><updated>2020-12-20T21:01:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/etl-airflow</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/etl-airflow/"><![CDATA[<p>ETL is an automated process that takes raw data, extracts and transforms the information required for analysis, and loads it to a data warehouse. There are different ways to build your ETL pipeline, on this post we’ll be using three main tools:</p> <ul> <li><strong>Airflow</strong>: one of the most powerful platforms used by Data Engineers for orchestrating workflows.</li> <li><strong>AWS ECS/Fargate</strong>: a container management service that makes it easy to run, stop, and manage your containers.</li> <li><strong>AWS s3</strong>: AWS simple storage service.</li> </ul> <p>The architecture that we will be building follows the schema bellow:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/ETL_architecture.png" alt="" title="ETL Architecture"/> </div> </div> <div class="caption"> Architecture schema, using AWS, Docker and Airflow. Designed using Lucid.app </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/building-an-etl-pipeline-with-airflow-and-ecs-4f68b7aa3b5b">Building an ETL pipeline with Airflow and ECS</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[ETL is an automated process that takes raw data, extracts and transforms the information required for analysis, and loads it to a data warehouse.]]></summary></entry><entry><title type="html">Going Bastion-less - Accessing Private EC2 instance with Session Manager</title><link href="https://danieldacosta.github.io/blog/2020/bastionless/" rel="alternate" type="text/html" title="Going Bastion-less - Accessing Private EC2 instance with Session Manager"/><published>2020-11-02T00:00:00+00:00</published><updated>2020-11-02T00:00:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/bastionless</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/bastionless/"><![CDATA[<p>In this post we will set up a private EC2 instance (in a private subnet), and use SSM session manager to access the instance that hosts a Jupyter Notebook server. We will then use PostForwarding with AWS Session Manager to access our server from our local machine.</p> <p>We’ll set up this infrastructure without opening inbound ports or setting up bastion hosts or managing SSH keys!.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/bastionless.png" alt="" title="Bastion-less Architecture"/> </div> </div> <div class="caption"> Bastion Host architecture in order to access the private instance. Designed using Lucid.app </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/going-bastion-less-accessing-private-ec2-instance-with-session-manager-c958cbf8489f">Going Bastion-less: Accessing Private EC2 instance with Session Manager</a></p> ]]></content><author><name></name></author><summary type="html"><![CDATA[In this post we will set up a private EC2 instance (in a private subnet), and use SSM session manager to access the instance that hosts a Jupyter Notebook server.]]></summary></entry><entry><title type="html">Discord notification using CloudWatch Alarms, SNS and AWS Lambda</title><link href="https://danieldacosta.github.io/blog/2020/notifications/" rel="alternate" type="text/html" title="Discord notification using CloudWatch Alarms, SNS and AWS Lambda"/><published>2020-09-12T15:12:00+00:00</published><updated>2020-09-12T15:12:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/notifications</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/notifications/"><![CDATA[<p>Alarms exist to notify us when our system behaves in an unexpected way, which warrants manual intervention to correct. When we have multiple systems in a production environment and an error passes unnoticed, the consequences can be catastrophic.</p> <p>An alarm should be created when the system cannot automatically recover, and human intervention is required. If an alert happens to occur too frequently it might lead to longer response time or even get missed.</p> <p>In this article, we will be building an alarm notification pipeline for an AWS Lambda function. For that will be using 3 AWS Services: AWS Lambda, Simple Notification Service (SNS), and CloudWatch. The goal is to send a notification to a Discord Channel when a CloudWatch Alarm is triggered.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/notifications.png" alt="" title="SNS Notifications Architecture"/> </div> </div> <div class="caption"> Designed using Lucidchart: https://www.lucidchart.com </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/discord-notification-using-cloudwatch-alarms-sns-and-aws-lambda-71393861699f">Discord notification using CloudWatch Alarms, SNS and AWS Lambda</a></p> ]]></content><author><name></name></author><summary type="html"><![CDATA[Alarms exist to notify us when our system behaves in an unexpected way, which warrants manual intervention to correct.]]></summary></entry><entry><title type="html">Automating Lambda modules deployment with GitLab CI</title><link href="https://danieldacosta.github.io/blog/2020/gitlabci/" rel="alternate" type="text/html" title="Automating Lambda modules deployment with GitLab CI"/><published>2020-08-09T15:59:00+00:00</published><updated>2020-08-09T15:59:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/gitlabci</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/gitlabci/"><![CDATA[<p>While working with terraform lambda modules, I had a hard time finding out the best repository architecture to automate my lambdas deployment. I couldn’t find any article that I could use as a guideline, that’s why I’m writing this article.</p> <p>While working on big projects, project organization is a must-have. In this story, you will be presented to one way to organize your repositories so that it facilitates the deployment procedure, making it much more scalable.</p> <p>In this article, we’ll be building the following repositories architecture:</p> <ul> <li><strong>Lambda-Module</strong>: Repository containing the Terraform Lambda module.</li> <li><strong>Lambda-Infra</strong>: Repository containing Terraform code for deployment into AWS.</li> <li><strong>Lambda-Code</strong>: Repository containing the lambda code for deployment using Gitlab-CI.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gitlab-ci.jpg" alt="" title="GItlab-ci Image"/> </div> </div> <div class="caption"> Photo by Sai Kiran Anagani on Unsplash </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/automating-lambda-modules-deployment-with-gitlab-ci-b34cc58a7ac0">Automating Lambda modules deployment with GitLab CI</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this story, you will be presented to one way to organize your repositories so that it facilitates the deployment procedure, making it much more scalable.]]></summary></entry><entry><title type="html">Building an ApiGateway-SQS-Lambda integration using Terraform</title><link href="https://danieldacosta.github.io/blog/2020/sqs/" rel="alternate" type="text/html" title="Building an ApiGateway-SQS-Lambda integration using Terraform"/><published>2020-06-22T15:09:00+00:00</published><updated>2020-06-22T15:09:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/sqs</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/sqs/"><![CDATA[<p>Terraform is an amazing tool for building infrastructures. This tool is used for building, changing, and versioning infrastructure safely and efficiently. Terraform is the infrastructure as code offering from HashiCorp.</p> <p>While using Terraform for building a project that I’m designing using Amazon Web Services (AWS), I came across the need to set up an API Gateway endpoint that takes records, put them into an SQS queue that triggers an Event Source for a Lambda function.</p> <p>In this post, I would like to share with you each step required to build this infrastructure. This post assumes that you are familiar with Terraform code and AWS services.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/sqs.png" alt="" title="SQS Architecture"/> </div> </div> <div class="caption"> Diagram made using LucidChart workspace: https://www.lucidchart.com/pages/?noHomepageRedirect=true </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/building-an-apigateway-sqs-lambda-integration-using-terraform-5617cc0408ad">Building an ApiGateway-SQS-Lambda integration using Terraform</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Terraform is an amazing tool for building infrastructures. This tool is used for building, changing, and versioning infrastructure safely and efficiently.]]></summary></entry><entry><title type="html">Text Classifier with Multiple Outputs and Multiple Losses in Keras</title><link href="https://danieldacosta.github.io/blog/2020/nlp/" rel="alternate" type="text/html" title="Text Classifier with Multiple Outputs and Multiple Losses in Keras"/><published>2020-05-09T21:01:00+00:00</published><updated>2020-05-09T21:01:00+00:00</updated><id>https://danieldacosta.github.io/blog/2020/nlp</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/nlp/"><![CDATA[<p>In this post, we’ll go through the definition of a multi-label classifier, multiple losses, text preprocessing and a step-by-step explanation on how to build a multi-output RNN-LSTM in Keras.</p> <p>The dataset that we’ll be working on consists of natural disaster messages that are classified into 36 different classes. The dataset was provided by Figure Eight. Example of input messages:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['Weather update - a cold front from Cuba that could pass over Haiti',
 'Is the Hurricane over or is it not over',
 'Looking for someone but no name',
 'UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.',
 'says: west side of Haiti, rest of the country today and tonight']
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/softmax.png" alt="" title="Softmax Architecture"/> </div> </div> <div class="caption"> Softmax Classification function in a Neural Network </div> <hr/> <p>You can check the full blog post on my Medium at Towards Data Science: <a href="https://towardsdatascience.com/text-classifier-with-multiple-outputs-and-multiple-losses-in-keras-4b7a527eb858">Text Classifier with Multiple Outputs and Multiple Losses in Keras</a></p> ]]></content><author><name></name></author><summary type="html"><![CDATA[Building a Multi-Label Classifier doesn't seem a difficult task using Keras, but when you are dealing with a highly imbalanced dataset with more than 30 different labels and with multiple losses it can become quite tricky.]]></summary></entry><entry><title type="html">Uploading a Python package to PyPi</title><link href="https://danieldacosta.github.io/blog/2020/pypi/" rel="alternate" type="text/html" title="Uploading a Python package to PyPi"/><published>2020-04-07T16:40:16+00:00</published><updated>2020-04-07T16:40:16+00:00</updated><id>https://danieldacosta.github.io/blog/2020/pypi</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/pypi/"><![CDATA[<p>As a Python developer, you must be familiar with the python packages installation procedure: <em>pip install {package_name}</em>. But maybe, you have never asked yourself how this works behind the curtain. This post is to briefly explain how you can upload you own package to PyPi, so that other programmers can use it.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/pip.png" alt="" title="Pypi"/> </div> </div> <div class="caption"> Image taken from: https://pypi.org </div> <hr/> <p>You can check the full blog post on my Medium at Python in Plain English: <a href="https://python.plainenglish.io/uploading-a-python-package-to-pypi-33e13ed6c6d3">Uploading a Python package to PyPi</a></p> ]]></content><author><name></name></author><summary type="html"><![CDATA[This post is to briefly explain how you can upload you own package to PyPi, so that other programmers can use it.]]></summary></entry><entry><title type="html">Rio de Janeiro Airbnb Data Analysis</title><link href="https://danieldacosta.github.io/blog/2020/airbnb/" rel="alternate" type="text/html" title="Rio de Janeiro Airbnb Data Analysis"/><published>2020-03-28T16:40:16+00:00</published><updated>2020-03-28T16:40:16+00:00</updated><id>https://danieldacosta.github.io/blog/2020/airbnb</id><content type="html" xml:base="https://danieldacosta.github.io/blog/2020/airbnb/"><![CDATA[<p>Working as a data scientist at a start-up, new projects and new demands are something that we should all be prepared for. My learning curve in the company keeps growing every day, and I thought that it would be nice to write down some of my personal works. What a better way to do that than publishing it on Medium ?</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/airbnb.png" alt="" title="Airbnb Logo"/> </div> </div> <hr/> <p>You can check the full blog post on my Medium: <a href="https://danielpdacosta.medium.com/rio-de-janeiro-airbnb-data-analysis-b43241102455">Rio de Janeiro Airbnb Data Analysis</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post, I’ll be analyzing a dataset from Airbnb website history for the city of Rio de Janeiro, Brazil.]]></summary></entry></feed>